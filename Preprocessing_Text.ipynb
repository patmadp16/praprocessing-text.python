{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Preprocessing_Text.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMcqzR+SK+b9Jwkh2XlADT2",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/patmadp16/praprocessing-text.python/blob/main/Preprocessing_Text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUPutYBOmzcm"
      },
      "source": [
        "# Connect google/drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDPHESA10LbK"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount ('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6B4wSjyMD1E1"
      },
      "source": [
        "# Read Dataset "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LgYS3y-52Ixa"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "#pd.set_option(\"max_columns\",None)\n",
        "#komentar = pd.read_csv(\"/content/drive/MyDrive/NLP/layanan.csv\",usecols=['Text'],encoding=\"ISO-8859-1\")\n",
        "#menampilkan data komentar kkb\n",
        "komentar=pd.read_csv('/content/drive/MyDrive/NLP/layanan.csv',error_bad_lines=False, engine=\"python\")\n",
        "pd.set_option('display.max_colwidth', -1)\n",
        "#using only v1 and v2 column\n",
        "#data.head()\n",
        "komentar.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JeKpPkXI0Fe"
      },
      "source": [
        "#mengecek data kosong\n",
        "komentar.isna().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KiBQmRJjJ2tB"
      },
      "source": [
        "#menghapus baris yang datanya kosong\n",
        "komentar.dropna(inplace=True)\n",
        "komentar.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7b9PNRb-gRm"
      },
      "source": [
        "#Case Folding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWzIYKQA2HPe"
      },
      "source": [
        "#------Case Folding------\n",
        "komentar['Comments_CaseFolding']=komentar['Text'].str.lower()\n",
        "komentar[['Comments_CaseFolding','Text']].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TcQR5j9vj8w3"
      },
      "source": [
        "import re\n",
        "def remove_pattern(text,pattern):\n",
        "  r = re.findall(pattern,text)\n",
        "  for i in r:\n",
        "    text = re.sub(i, '' , text)\n",
        "  return text\n",
        "\n",
        "komentar['Comments_CaseFolding']=np.vectorize(remove_pattern)(komentar['Comments_CaseFolding'],\"#[\\w]*\")\n",
        "komentar['Comments_CaseFolding']=np.vectorize(remove_pattern)(komentar['Comments_CaseFolding'],\"@[\\w]*\")\n",
        "komentar[['Comments_CaseFolding','Text']].head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sqigeu_bkjRE"
      },
      "source": [
        "#remove url\n",
        "import re\n",
        "def clean_links(text):\n",
        "  text = re.sub(r\"http\\S+\", \"\", text)\n",
        "  return text\n",
        "\n",
        "komentar['Comments_Text']=komentar['Comments_CaseFolding'].apply(clean_links)\n",
        "komentar[['Comments_Text','Comments_CaseFolding']].head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSMsyhWqDNOE"
      },
      "source": [
        "# Remove Char, Punctuation, DoubleWord\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdd199q-G2yf"
      },
      "source": [
        "import re\n",
        "def remove_emoticon(text):\n",
        "  text = text.encode('ascii',\"replace\").decode('ascii')\n",
        "  return text\n",
        "\n",
        "komentar['Comments_dText'] = komentar['Comments_Text']  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2aeIU55DnL0"
      },
      "source": [
        "Remove Char, Number, Double World"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGyS2r6Ll2rV"
      },
      "source": [
        "\n",
        "#text cleaning\n",
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "  #Remove RT/ b'\n",
        "  text = text.replace(\"b'\",\"\")\n",
        "\n",
        "  #Remove \\n, \\t, \\r\n",
        "  text = re.sub(r\"\\\\n\",\"\",text)\n",
        "  text = re.sub(r\"\\\\r\",\" \",text)\n",
        "  text = re.sub(r\"\\\\t\",\" \",text)\n",
        "\n",
        "  text = re.sub(\"\\s+\",\" \",text)\n",
        "\n",
        "  #Mengganti &amp dengan &\n",
        "  text = re.sub(r\"&amp;\",\" \",text)\n",
        "  \n",
        "  text=text.lower()\n",
        "\n",
        "  #Remove tanda baca\n",
        "  text = re.sub(r\"[?!.,;:#@_+()|\\/<>]\",\"\",text)\n",
        "  text = text.replace(\"https\", '').replace(\"http\",'')\n",
        "\n",
        "  return text\n",
        "  \n",
        "def remove_emoticon(text):\n",
        "  text = text.encode('ascii',\"replace\").decode('ascii')\n",
        "  return text  \n",
        "\n",
        "\n",
        "#tweets = remove_emoticon(dataset.text.str)\n",
        "komentar['Comments_Text'] = komentar['Comments_Text'].apply(clean_text)\n",
        "#komentar['Comments_Text']= komentar['Comments_Text'].apply(remove_emoticon2)\n",
        "komentar['Comments_Text']= komentar['Comments_Text'].str.replace(\"[^a-zA-Z#][\\a]\",\" \")\n",
        "#komentar['Comments_Text']= komentar['Comments_Text'].str.replace(u'\\xe2','a')\n",
        "\n",
        "komentar[['Comments_Text','Comments_CaseFolding']].head(5) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3A9vGo8C2Fd"
      },
      "source": [
        "import string \n",
        "import re #regex library\n",
        "\n",
        "# import word_tokenize & FreqDist from NLTK\n",
        "from nltk.tokenize import word_tokenize \n",
        "from nltk.probability import FreqDist\n",
        "\n",
        "#library that contains punctuation\n",
        "string.punctuation\n",
        "\n",
        "def remove_char(text):\n",
        "  return text.replace(\".\", \" \")\n",
        "\n",
        "#Menghapus tanda baca\n",
        "def remove_punctuation(text):\n",
        "    punctuation_comments=\"\".join([i for i in text if i not in string.punctuation])\n",
        "    text.replace(punctuation_comments,\" \")\n",
        "    return punctuation_comments\n",
        "\n",
        "#Meghapus emoji\n",
        "def deEmojify(text):\n",
        "    return text.encode('ascii', 'ignore').decode('ascii')\n",
        "\n",
        "#Menghapus angka\n",
        "def remove_number(text):\n",
        "    return  re.sub(r\"\\d+\", \"\", text)\n",
        "\n",
        "komentar['RemoveChar'] = komentar['Comments_Text'].apply(remove_char).apply(lambda x:remove_punctuation(x)).apply(remove_number).apply(deEmojify)\n",
        "komentar[['RemoveChar','Comments_Text']].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KRd-ZqOeq-j"
      },
      "source": [
        "# Menghapus double word\n",
        "from collections import OrderedDict\n",
        "komentar['RemoveDoubleWord'] = (komentar['RemoveChar'].str.split().apply(lambda x: OrderedDict.fromkeys(x).keys()).str.join(' '))\n",
        "\n",
        "komentar[['RemoveChar','RemoveDoubleWord']].head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCHNpUkIn01O"
      },
      "source": [
        "# Tokenizing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PfIYGa0RWIBO"
      },
      "source": [
        "#defining function for tokenization\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "def tokenization(text):\n",
        "    tokens = re.split('W+',text)\n",
        "    return tokens\n",
        "\n",
        "def word_tokenize_wrapper(text):\n",
        "    return word_tokenize(text)\n",
        "\n",
        "#applying function to the column\n",
        "komentar['tokenizing']= komentar['RemoveDoubleWord'].apply(word_tokenize_wrapper)\n",
        "komentar[['tokenizing','RemoveDoubleWord']].head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ot_h8g-T_dw_"
      },
      "source": [
        "# Normalisasi slang word"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vG_eKnYL_Z7A"
      },
      "source": [
        "normalizad_word = pd.read_excel(\"/content/drive/MyDrive/NLP/Normalisasi_word.xlsx\")\n",
        "\n",
        "normalizad_word_dict = {}\n",
        "\n",
        "for index, row in normalizad_word.iterrows():\n",
        "    if row[0] not in normalizad_word_dict:\n",
        "        normalizad_word_dict[row[0]] = row[1] \n",
        "\n",
        "def normalized_term(document):\n",
        "    return [normalizad_word_dict[term] if term in normalizad_word_dict else term for term in document]\n",
        "\n",
        "komentar['Normal_token'] = komentar['tokenizing'].apply(normalized_term)\n",
        "\n",
        "komentar[['tokenizing','Normal_token']].head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqzFYrkypYV1"
      },
      "source": [
        "# Stopword"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4W4gSIkLhkMu"
      },
      "source": [
        "#import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "# ----------------------- get stopword from NLTK stopword -------------------------------\n",
        "# get stopword indonesia\n",
        "list_stopwords = stopwords.words('indonesian')\n",
        "\n",
        "# read txt stopword using pandas\n",
        "txt_stopword = pd.read_csv(\"/content/drive/MyDrive/NLP/stopword_combine.txt\", names= [\"stopwords\"],header = None)\n",
        "\n",
        "# convert stopword string to list & append additional stopword\n",
        "list_stopwords.extend(txt_stopword[\"stopwords\"][0].split(' '))\n",
        "\n",
        "# convert list to dictionary\n",
        "list_stopwords = set(list_stopwords)\n",
        "\n",
        "#remove stopword pada list token\n",
        "def stopwords_removal(words):\n",
        "  output = [i for i in words if i not in list_stopwords]\n",
        "  return output\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    output= [i for i in text if i not in stopwords_english]\n",
        "    return output\n",
        "\n",
        "komentar['Comments_stopwordfix'] =komentar['Normal_token'].apply(stopwords_removal)\n",
        "komentar[[\"Normal_token\",\"Comments_stopwordfix\"]].head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTLoWY79I7Lq"
      },
      "source": [
        "!pip install Sastrawi\n",
        "# import StemmerFactory class\n",
        "#importing the Stemming function from nltk library\n",
        "#from nltk.stem.porter import PorterStemmer\n",
        "#defining the object for stemming\n",
        "#porter_stemmer = PorterStemmer()\n",
        "#defining a function for stemming\n",
        "#def stemming(text):\n",
        "#  output = [porter_stemmer.stem(word) for word in text]\n",
        "#  return output\n",
        "#data['msg_stemmed']=data['no_sw_msg'].apply(lambda x: stemming(x))\n",
        "#komentar['Stemming'] =komentar['Comments_stopwordfix'].apply(lambda x: stemming(x))\n",
        "\n",
        "#komentar.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exNhHyTPNigu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87a864d2-d2f0-4d12-9bb8-4310eefe7e2f"
      },
      "source": [
        "# import Sastrawi package\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "#import swifter\n",
        "\n",
        "# create stemmer\n",
        "factory = StemmerFactory()\n",
        "stemmer = factory.create_stemmer()\n",
        "\n",
        "# stemmed\n",
        "def stemmed_wrapper(term):\n",
        "    return stemmer.stem(term)\n",
        "\n",
        "term_dict = {}\n",
        "for document in komentar['Comments_stopwordfix']:\n",
        "    for term in document:\n",
        "        if term not in term_dict:\n",
        "            term_dict[term] = ' '\n",
        "            \n",
        "print(len(term_dict))\n",
        "\n",
        "for term in term_dict:\n",
        "    term_dict[term] = stemmed_wrapper(term)\n",
        "# apply stemmed term to dataframe\n",
        "def get_stemmed_term(document):\n",
        "    return [term_dict[term] for term in document]\n",
        "\n",
        "\n",
        "komentar['Stemming2'] =komentar['Comments_stopwordfix'].apply(get_stemmed_term)\n",
        "komentar[['Comments_stopwordfix','Stemming2']].head(10)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "7115\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOxXOtC0EOuY"
      },
      "source": [
        "komentar.to_csv(\"comment_kkb_process.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}